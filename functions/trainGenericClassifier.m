function [trainedClassifier, validationStruct] = trainGenericClassifier(trainingData, classifier_options)
% trainClassifier(trainingData)
%  returns a trained classifier and its accuracy.
%  This code recreates the classification model trained in
%  Classification Learner app.
%
%   Input:
%       trainingData: the training data of same data type as imported
%        in the app (table or matrix).
%
%   Output:
%       trainedClassifier: a struct containing the trained classifier.
%        The struct contains various fields with information about the
%        trained classifier.
%
%       trainedClassifier.predictFcn: a function to make predictions
%        on new data. It takes an input of the same form as this training
%        code (table or matrix) and returns predictions for the response.
%        If you supply a matrix, include only the predictors columns (or
%        rows).
%
%       validationAccuracy: a double containing the accuracy in
%        percent. In the app, the History list displays this
%        overall accuracy score for each model.
%
%  Use the code to train the model with new data.
%  To retrain your classifier, call the function from the command line
%  with your original data or new data as the input argument trainingData.
%
%  For example, to retrain a classifier trained with the original data set
%  T, enter:
%    [trainedClassifier, validationAccuracy] = trainClassifier(T)
%
%  To make predictions with the returned 'trainedClassifier' on new data T,
%  use
%    yfit = trainedClassifier.predictFcn(T)
%
%  To automate training the same classifier with new data, or to learn how
%  to programmatically train classifiers, examine the generated code.

% Auto-generated by MATLAB on 15-Feb-2017 10:13:17
% Modified on 15-Feb-2017 by AJS to fit a generic classifier specified by
% the parameters in "classifier_options"
%           input table should have two entries: a matrix of predictors
%               called table_x and a vector of responses called table_y
%   classifier_options.classifier_type should be 'ensembleSubspace',
%   'gaussianSVM', or 'KNN' (case-insensitive) and can optionally have a
%   field that sets some predictors to categorical predictors
%   (isCategoricalPredictor). This value is 'false' by default. 

% predictor string
predictor_string = 'table_x';

% Extract predictors and response
% This code processes the data into the right shape for training the
% classifier.
inputTable = trainingData;

% guess the number of variables
[~, nC] = size(inputTable{1, :});
num_predictors = nC - 1;

if num_predictors > 99
    pN1 = cellstr(num2str((1:9)', [predictor_string '_%d']))';
    pN2 = cellstr(num2str((10:99)', [predictor_string '_%d']))';
    pN3 = cellstr(num2str((100:num_predictors)', [predictor_string '_%d']))';
    predictorNames = [pN1 pN2 pN3];
elseif num_predictors > 9
    pN1 = cellstr(num2str((1:9)', [predictor_string '_%d']))';
    pN2 = cellstr(num2str((10:num_predictors)', [predictor_string '_%d']))';
    predictorNames = [pN1 pN2];    
else
    predictorNames = cellstr(num2str((1:num_predictors)', [predictor_string '_%d']))';
end 
% Split matrices in the input table into vectors
inputTable = [inputTable(:,setdiff(inputTable.Properties.VariableNames, {predictor_string})), ...
    array2table(table2array(inputTable(:,{predictor_string})), 'VariableNames', predictorNames)];
  
predictors = inputTable(:, predictorNames); 
response = inputTable.table_y;
class_names = unique(response);


if isfield(classifier_options, 'isCategoricalPredictor')
    isCategoricalPredictor = classifier_options.isCategoricalPredictor;
else
    isCategoricalPredictor = false(1, num_predictors);
end

if classifier_options.applyPCA
    
    % Apply a PCA to the predictor matrix.
    % Run PCA on numeric predictors only. Categorical predictors are passed through PCA untouched.
    isCategoricalPredictorBeforePCA = isCategoricalPredictor;
    numericPredictors = predictors(:, ~isCategoricalPredictor);
    numericPredictors = table2array(varfun(@double, numericPredictors));
    % 'inf' values have to be treated as missing data for PCA.
    numericPredictors(isinf(numericPredictors)) = NaN;
    [pcaCoefficients, pcaScores, ~, ~, explained, pcaCenters] = pca(...
        numericPredictors, ...
        'Centered', false);
    % Keep enough components to explain the desired amount of variance.
    explainedVarianceToKeepAsFraction = 95/100;
    numComponentsToKeep = find(cumsum(explained)/sum(explained) >= explainedVarianceToKeepAsFraction, 1);
    pcaCoefficients = pcaCoefficients(:,1:numComponentsToKeep);
    predictors = [array2table(pcaScores(:,1:numComponentsToKeep)), predictors(:, isCategoricalPredictor)];
    
end



% Train a classifier
if strcmpi(classifier_options.classifier_type, 'ensembleSubspace')
    % default parameters for ensemble subspace discriminant classifier
    subspaceDimension = max(1, min(2, width(predictors) - 1));
    classification_fit = fitensemble(...
        predictors, ...
        response, ...
        'Subspace', ...
        30, ...
        'Discriminant', ...
        'Type', 'Classification', ...
        'NPredToSample', subspaceDimension, ...
        'ClassNames', class_names);
elseif strcmpi(classifier_options.classifier_type, 'gaussianSVM')
    % default options for the medium gaussian svm classifier
    if isfield(classifier_options, 'kernel_scale') 
        kernel_scale = classifier_options.kernel_scale;
    else
        kernel_scale = 3;
    end
    
    if isfield(classifier_options, 'boxconstraint')
        box_constraint = classifier_options.boxconstraint;
    else
        box_constraint = 1;
    end
    
    
    template = templateSVM(...
        'KernelFunction', 'gaussian', ...
        'PolynomialOrder', [], ...
        'KernelScale', kernel_scale, ...
        'BoxConstraint', box_constraint, ...
        'Standardize', true);
    classification_fit = fitcecoc(...
        predictors, ...
        response, ...
        'Learners', template, ...
        'Coding', 'onevsone', ...
        'ClassNames', class_names);
elseif strcmpi(classifier_options.classifier_type, 'linearSVM')

    if isfield(classifier_options, 'boxconstraint')
        box_constraint = classifier_options.boxconstraint;
    else
        box_constraint = 1;
    end
    
    
    template = templateSVM(...
        'KernelFunction', 'linear', ...
        'PolynomialOrder', [], ...
        'BoxConstraint', box_constraint, ...
        'Standardize', true);
    classification_fit = fitcecoc(...
        predictors, ...
        response, ...
        'Learners', template, ...
        'Coding', 'onevsone', ...
        'ClassNames', class_names);    
elseif strcmpi(classifier_options.classifier_type, 'KNN')
    % default options for fine KNN classifier
    classification_fit = fitcknn(...
        predictors, ...
        response, ...
        'Distance', 'Euclidean', ...
        'Exponent', [], ...
        'NumNeighbors', 1, ...
        'DistanceWeight', 'Equal', ...
        'Standardize', true, ...
        'ClassNames', class_names);
elseif strcmpi(classifier_options.classifier_type, 'LDA')
    
    classification_fit = fitcdiscr(...
        predictors, ...
        response, ...
        'DiscrimType', 'linear', ...
        'Gamma', 0, ...
        'FillCoeffs', 'off', ...
        'ClassNames', class_names);
elseif strcmpi(classifier_options.classifier_type, 'pLDA')
%      uses pseudoinverse 

    classification_fit = fitcdiscr(...
        predictors, ...
        response, ...
        'DiscrimType', 'pseudolinear', ...
        'Gamma', 0.02, ...
        'FillCoeffs', 'off', ...
        'ClassNames', class_names);

end


if classifier_options.applyPCA
    % Create the result struct with predict function
    splitMatricesInTableFcn = @(t) [t(:,setdiff(t.Properties.VariableNames, {'table_x'})), array2table(table2array(t(:,{'table_x'})), 'VariableNames', {'table_x_1', 'table_x_2', 'table_x_3', 'table_x_4', 'table_x_5', 'table_x_6', 'table_x_7', 'table_x_8', 'table_x_9', 'table_x_10', 'table_x_11', 'table_x_12', 'table_x_13', 'table_x_14', 'table_x_15', 'table_x_16', 'table_x_17', 'table_x_18', 'table_x_19', 'table_x_20', 'table_x_21', 'table_x_22', 'table_x_23', 'table_x_24', 'table_x_25', 'table_x_26', 'table_x_27', 'table_x_28', 'table_x_29', 'table_x_30', 'table_x_31', 'table_x_32', 'table_x_33', 'table_x_34', 'table_x_35', 'table_x_36', 'table_x_37', 'table_x_38', 'table_x_39', 'table_x_40', 'table_x_41', 'table_x_42', 'table_x_43', 'table_x_44', 'table_x_45', 'table_x_46', 'table_x_47', 'table_x_48', 'table_x_49', 'table_x_50', 'table_x_51', 'table_x_52', 'table_x_53', 'table_x_54', 'table_x_55', 'table_x_56', 'table_x_57', 'table_x_58', 'table_x_59', 'table_x_60'})];
    extractPredictorsFromTableFcn = @(t) t(:, predictorNames);
    predictorExtractionFcn = @(x) extractPredictorsFromTableFcn(splitMatricesInTableFcn(x));
    pcaTransformationFcn = @(x) [ array2table(bsxfun(@minus, table2array(varfun(@double, x(:, ~isCategoricalPredictorBeforePCA))), pcaCenters) * pcaCoefficients), x(:,isCategoricalPredictorBeforePCA) ];
    ensemblePredictFcn = @(x) predict(classification_fit, x);
    trainedClassifier.predictFcn = @(x) ensemblePredictFcn(pcaTransformationFcn(predictorExtractionFcn(x)));

    % Add additional fields to the result struct
    trainedClassifier.RequiredVariables = {'table_x'};
    trainedClassifier.PCACenters = pcaCenters;
    trainedClassifier.PCACoefficients = pcaCoefficients;
    trainedClassifier.ClassificationDiscriminant = classification_fit;
    trainedClassifier.About = 'This struct is a trained classifier exported from Classification Learner R2016a.';
    trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedClassifier''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');
else
    % Create the result struct with predict function
    splitMatricesInTableFcn = @(t) [t(:,setdiff(t.Properties.VariableNames, ...
        {predictor_string})), array2table(table2array(t(:,{predictor_string})), ...
        'VariableNames', predictorNames)];
    extractPredictorsFromTableFcn = @(t) t(:, predictorNames);
    predictorExtractionFcn = @(x) extractPredictorsFromTableFcn(splitMatricesInTableFcn(x));
    ensemblePredictFcn = @(x) predict(classification_fit, x);
    trainedClassifier.predictFcn = @(x) ensemblePredictFcn(predictorExtractionFcn(x));

    % Add additional fields to the result struct
    trainedClassifier.RequiredVariables = {predictor_string};
    if classifier_options.full_save
        trainedClassifier.ClassificationEnsemble = classification_fit;
    end
    trainedClassifier.About = 'This struct is a trained classifier using Matlab classifier fitting functions and custom routines.';
    trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedClassifier''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');
end




if classifier_options.applyPCA
        % Extract predictors and response
    % This code processes the data into the right shape for training the
    % classifier.
    inputTable = trainingData;
    % Split matrices in the input table into vectors
    inputTable = [inputTable(:,setdiff(inputTable.Properties.VariableNames, {predictor_string})), ...
        array2table(table2array(inputTable(:,{predictor_string})), ...
        'VariableNames', predictorNames)]; 

    predictors = inputTable(:, predictorNames);
    response = inputTable.table_y;

   % Perform cross-validation
    KFolds = 5;
    cvp = cvpartition(response, 'KFold', KFolds);
    % Initialize the predictions and scores to the proper sizes
    validationPredictions = response;
    numObservations = size(predictors, 1);
    numClasses = length(class_names);
    validationScores = NaN(numObservations, numClasses);
    for fold = 1:KFolds
        trainingPredictors = predictors(cvp.training(fold), :);
        trainingResponse = response(cvp.training(fold), :);
        foldIsCategoricalPredictor = isCategoricalPredictor;

        % Apply a PCA to the predictor matrix.
        % Run PCA on numeric predictors only. Categorical predictors are passed through PCA untouched.
        isCategoricalPredictorBeforePCA = foldIsCategoricalPredictor;
        numericPredictors = trainingPredictors(:, ~foldIsCategoricalPredictor);
        numericPredictors = table2array(varfun(@double, numericPredictors));
        % 'inf' values have to be treated as missing data for PCA.
        numericPredictors(isinf(numericPredictors)) = NaN;
        [pcaCoefficients, pcaScores, ~, ~, explained, pcaCenters] = pca(...
            numericPredictors, ...
            'Centered', false);
        % Keep enough components to explain the desired amount of variance.
        explainedVarianceToKeepAsFraction = 95/100;
        numComponentsToKeep = find(cumsum(explained)/sum(explained) >= explainedVarianceToKeepAsFraction, 1);
        pcaCoefficients = pcaCoefficients(:,1:numComponentsToKeep);
        trainingPredictors = [array2table(pcaScores(:,1:numComponentsToKeep)), trainingPredictors(:, foldIsCategoricalPredictor)];
        foldIsCategoricalPredictor = [false(1,numComponentsToKeep), true(1,sum(foldIsCategoricalPredictor))];

        % Train a classifier
        % This code specifies all the classifier options and trains the classifier.
        if strcmpi(classifier_options.classifier_type, 'ensembleSubspace')
            % default parameters for ensemble subspace discriminant classifier
            subspaceDimension = max(1, min(2, width(trainingPredictors) - 1));
            classification_fit = fitensemble(...
                trainingPredictors, ...
                trainingResponse, ...
                'Subspace', ...
                30, ...
                'Discriminant', ...
                'Type', 'Classification', ...
                'NPredToSample', subspaceDimension, ...
                'ClassNames', class_names);
        elseif strcmpi(classifier_options.classifier_type, 'gaussianSVM')
                % default options for the medium gaussian svm classifier
            if isfield(classifier_options, 'kernel_scale') 
                kernel_scale = classifier_options.kernel_scale;
            else
                kernel_scale = 3;
            end

            if isfield(classifier_options, 'boxconstraint')
                box_constraint = classifier_options.boxconstraint;
            else
                box_constraint = 1;
            end

            
            
            
            template = templateSVM(...
                'KernelFunction', 'gaussian', ...
                'PolynomialOrder', [], ...
                'KernelScale', kernel_scale, ...
                'BoxConstraint', box_constraint, ...
                'Standardize', true);
            classification_fit = fitcecoc(...
                trainingPredictors, ...
                trainingResponse, ...
                'Learners', template, ...
                'Coding', 'onevsone', ...
                'ClassNames', class_names);
            
            
            
        elseif strcmpi(classifier_options.classifier_type, 'linearSVM')
                % default options for the linear svm classifier
%             if isfield(classifier_options, 'kernel_scale') 
%                 kernel_scale = classifier_options.kernel_scale;
%             else
%                 kernel_scale = 3;
%             end

            if isfield(classifier_options, 'boxconstraint')
                box_constraint = classifier_options.boxconstraint;
            else
                box_constraint = 1;
            end

            
            
            
            template = templateSVM(...
                'KernelFunction', 'linear', ...
                'BoxConstraint', box_constraint, ...
                'Standardize', true);
            classification_fit = fitcecoc(...
                trainingPredictors, ...
                trainingResponse, ...
                'Learners', template, ...
                'Coding', 'onevsone', ...
                'ClassNames', class_names);

        elseif strcmpi(classifier_options.classifier_type, 'KNN')
            % default options for fine KNN classifier
            classification_fit = fitcknn(...
                trainingPredictors, ...
                trainingResponse, ...
                'Distance', 'Euclidean', ...
                'Exponent', [], ...
                'NumNeighbors', 1, ...
                'DistanceWeight', 'Equal', ...
                'Standardize', true, ...
                'ClassNames', class_names);
            
            
        elseif strcmpi(classifier_options.classifier_type, 'LDA')
    
            classification_fit = fitcdiscr(...
                trainingPredictors, ...
                trainingResponse, ...
                'DiscrimType', 'linear', ...
                'Gamma', 0, ...
                'FillCoeffs', 'off', ...
                'ClassNames', class_names);
            
        elseif strcmpi(classifier_options.classifier_type, 'pLDA')
    
            classification_fit = fitcdiscr(...
                trainingPredictors, ...
                trainingResponse, ...
                'DiscrimType', 'pseudolinear', ...
                'Gamma', 0, ...
                'FillCoeffs', 'off', ...
                'ClassNames', class_names);

        end


        % Create the result struct with predict function
        pcaTransformationFcn = @(x) [ array2table(bsxfun(@minus, table2array(varfun(@double, x(:, ~isCategoricalPredictorBeforePCA))), pcaCenters) * pcaCoefficients), x(:,isCategoricalPredictorBeforePCA) ];
        discriminantPredictFcn = @(x) predict(classification_fit, x);
        validationPredictFcn = @(x) discriminantPredictFcn(pcaTransformationFcn(x));

        % Add additional fields to the result struct

        % Compute validation predictions and scores
        validationPredictors = predictors(cvp.test(fold), :);
        [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);

        % Store predictions and scores in the original order
        validationPredictions(cvp.test(fold), :) = foldPredictions;
        validationScores(cvp.test(fold), :) = foldScores;
        
        % calculate validation accuracy
        correctPredictions = (validationPredictions == response);
        validationAccuracy = sum(correctPredictions)/length(correctPredictions);
    end 
    
else
    % Perform cross-validation
    partitionedModel = crossval(classification_fit, 'KFold', 5);

    % Compute validation accuracy
    validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

    % Compute validation predictions and scores
    [validationPredictions, validationScores] = kfoldPredict(partitionedModel);
end

validationStruct.validationAccuracy = validationAccuracy;
validationStruct.validationPredictions = validationPredictions;
validationStruct.validationScores = validationScores;
if classifier_options.full_save
    validationStruct.partitionedModel = partitionedModel;
end